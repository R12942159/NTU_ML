{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/R12942159/NTU_ML/blob/Hw4/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybs7iymbWbER"
      },
      "source": [
        "import torch\n",
        "import os\n",
        "import csv\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle competitions download -c ml-2023-fall-hw3\n",
        "!unzip 'ml-2023-fall-hw3.zip'"
      ],
      "metadata": {
        "id": "YzEiwyE78IkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VX5NXp4WbEi"
      },
      "source": [
        "DEVICE_NUM = 2\n",
        "BATCH_SIZE = 128\n",
        "EPOCH_NUM = 50\n",
        "MAX_POSITIONS_LEN = 500\n",
        "SEED = 5566\n",
        "MODEL_DIR = 'model.pth'\n",
        "lr = 0.001\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# torch.cuda.set_device(0)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "w2v_config = {'path': 'w2v.model', 'dim': 256}\n",
        "lstm_config = {'hidden_dim': 256, 'num_layers': 2, 'bidirectional': True, 'fix_embedding': True}\n",
        "header_config = {'dropout': 0.5, 'hidden_dim': 512}\n",
        "assert header_config['hidden_dim'] == lstm_config['hidden_dim'] or header_config['hidden_dim'] == lstm_config['hidden_dim'] * 2"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7PZMBJSWbEk"
      },
      "source": [
        "def parsing_text(text):\n",
        "    # TODO: do data processing\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "def load_train_label(path='train_label.csv'):\n",
        "    tra_lb_pd = pd.read_csv(path)\n",
        "    label = torch.FloatTensor(tra_lb_pd['label'].values)\n",
        "    idx = tra_lb_pd['id'].tolist()\n",
        "    text = [parsing_text(s).split(' ') for s in tra_lb_pd['text'].tolist()]\n",
        "    return idx, text, label\n",
        "\n",
        "def load_train_nolabel(path='train_nolabel.csv'):\n",
        "    tra_nlb_pd = pd.read_csv(path)\n",
        "    text = [parsing_text(s).split(' ') for s in tra_nlb_pd['text'].tolist()]\n",
        "    return text\n",
        "\n",
        "def load_test(path='test.csv'):\n",
        "    tst_pd = pd.read_csv(path)\n",
        "    idx = tst_pd['id'].tolist()\n",
        "    text = [parsing_text(s).split(' ') for s in tst_pd['text'].tolist()]\n",
        "    return idx, text"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6JVOfd0WbEo"
      },
      "source": [
        "class Preprocessor:\n",
        "    def __init__(self, sentences, w2v_config):\n",
        "        self.sentences = sentences\n",
        "        self.idx2word = []\n",
        "        self.word2idx = {}\n",
        "        self.embedding_matrix = []\n",
        "        self.build_word2vec(sentences, **w2v_config)\n",
        "\n",
        "    def build_word2vec(self, x, path, dim):\n",
        "        if os.path.isfile(path):\n",
        "            print(\"loading word2vec model ...\")\n",
        "            w2v_model = Word2Vec.load(path)\n",
        "        else:\n",
        "            print(\"training word2vec model ...\")\n",
        "            w2v_model = Word2Vec(x, vector_size=dim, window=5, min_count=2, workers=12, epochs=2, sg=1)\n",
        "            print(\"saving word2vec model ...\")\n",
        "            w2v_model.save(path)\n",
        "\n",
        "        self.embedding_dim = w2v_model.vector_size\n",
        "        for i, word in enumerate(w2v_model.wv.key_to_index):\n",
        "            #e.g. self.word2index['he'] = 1\n",
        "            #e.g. self.index2word[1] = 'he'\n",
        "            #e.g. self.vectors[1] = 'he' vector\n",
        "\n",
        "            self.word2idx[word] = len(self.word2idx)\n",
        "            self.idx2word.append(word)\n",
        "            self.embedding_matrix.append(w2v_model.wv[word])\n",
        "\n",
        "\n",
        "        self.embedding_matrix = torch.tensor(self.embedding_matrix)\n",
        "        self.add_embedding('<PAD>')\n",
        "        self.add_embedding('<UNK>')\n",
        "        print(\"total words: {}\".format(len(self.embedding_matrix)))\n",
        "\n",
        "    def add_embedding(self, word):\n",
        "        # 把 word 加進 embedding，並賦予他一個隨機生成的 representation vector\n",
        "        # word 只會是 \"<PAD>\" 或 \"<UNK>\"\n",
        "        vector = torch.empty(1, self.embedding_dim)\n",
        "        torch.nn.init.uniform_(vector)\n",
        "        self.word2idx[word] = len(self.word2idx)\n",
        "        self.idx2word.append(word)\n",
        "        self.embedding_matrix = torch.cat([self.embedding_matrix, vector], 0)\n",
        "\n",
        "    def sentence2idx(self, sentence):\n",
        "        sentence_idx = []\n",
        "        for word in sentence:\n",
        "            if word in self.word2idx.keys():\n",
        "                sentence_idx.append(self.word2idx[word])\n",
        "            else:\n",
        "                sentence_idx.append(self.word2idx[\"<UNK>\"])\n",
        "        return torch.LongTensor(sentence_idx)\n",
        "\n",
        "class TwitterDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, id_list, sentences, labels, preprocessor):\n",
        "        self.id_list = id_list\n",
        "        self.sentences = sentences\n",
        "        self.labels = labels\n",
        "        self.preprocessor = preprocessor\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.labels is None: return self.id_list[idx], self.preprocessor.sentence2idx(self.sentences[idx])\n",
        "        return self.id_list[idx], self.preprocessor.sentence2idx(self.sentences[idx]), self.labels[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def collate_fn(self, data):\n",
        "        id_list = torch.LongTensor([d[0] for d in data])\n",
        "        lengths = torch.LongTensor([len(d[1]) for d in data])\n",
        "        texts = pad_sequence(\n",
        "            [d[1] for d in data], batch_first=True).contiguous()\n",
        "\n",
        "        if self.labels is None:\n",
        "            return id_list, lengths, texts\n",
        "\n",
        "        labels = torch.FloatTensor([d[2] for d in data])\n",
        "        return id_list, lengths, texts, labels"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpK_-m5pWbGZ",
        "outputId": "f944909c-767f-4eaf-fd86-a530b6144fba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_idx, train_label_text, label = load_train_label('data/train_label.csv')\n",
        "preprocessor = Preprocessor(train_label_text, w2v_config)\n",
        "\n",
        "train_idx, valid_idx, train_label_text, valid_label_text, train_label, valid_label = train_test_split(train_idx, train_label_text, label, test_size=0.12)\n",
        "train_dataset, valid_dataset = TwitterDataset(train_idx, train_label_text, train_label, preprocessor), TwitterDataset(valid_idx, valid_label_text, valid_label, preprocessor)\n",
        "\n",
        "test_idx, test_text = load_test('data/test.csv')\n",
        "test_dataset = TwitterDataset(test_idx, test_text, None, preprocessor)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
        "                                            batch_size = BATCH_SIZE,\n",
        "                                            shuffle = True,\n",
        "                                            collate_fn = train_dataset.collate_fn,\n",
        "                                            num_workers = 8)\n",
        "valid_loader = torch.utils.data.DataLoader(dataset = valid_dataset,\n",
        "                                            batch_size = BATCH_SIZE,\n",
        "                                            shuffle = False,\n",
        "                                            collate_fn = valid_dataset.collate_fn,\n",
        "                                            num_workers = 8)\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
        "                                            batch_size = BATCH_SIZE,\n",
        "                                            shuffle = False,\n",
        "                                            collate_fn = valid_dataset.collate_fn,\n",
        "                                            num_workers = 8)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training word2vec model ...\n",
            "saving word2vec model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-39f39bb9485d>:30: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
            "  self.embedding_matrix = torch.tensor(self.embedding_matrix)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total words: 30885\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BtL7itbWbGk"
      },
      "source": [
        "class LSTM_Backbone(torch.nn.Module):\n",
        "    def __init__(self, embedding, hidden_dim, num_layers, bidirectional, fix_embedding=True):\n",
        "        super(LSTM_Backbone, self).__init__()\n",
        "        self.embedding = torch.nn.Embedding(embedding.size(0),embedding.size(1))\n",
        "        self.embedding.weight = torch.nn.Parameter(embedding)\n",
        "        self.embedding.weight.requires_grad = False if fix_embedding else True\n",
        "\n",
        "        self.lstm = torch.nn.LSTM(embedding.size(1), hidden_dim, num_layers=num_layers, \\\n",
        "                                  bidirectional=bidirectional, batch_first=True)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        inputs = self.embedding(inputs)\n",
        "        x, _ = self.lstm(inputs)\n",
        "        return x\n",
        "\n",
        "class Header(torch.nn.Module):\n",
        "    def __init__(self, dropout, hidden_dim):\n",
        "        super(Header, self).__init__()\n",
        "        # TODO: you should design your classifier module\n",
        "        self.classifier = torch.nn.Sequential(torch.nn.Dropout(dropout),\n",
        "                                         torch.nn.Linear(hidden_dim, 1),\n",
        "                                         torch.nn.Sigmoid())\n",
        "\n",
        "    @ torch.no_grad()\n",
        "    def _get_length_masks(self, lengths):\n",
        "        # lengths: (batch_size, ) in cuda\n",
        "        ascending = torch.arange(MAX_POSITIONS_LEN)[:lengths.max().item()].unsqueeze(\n",
        "            0).expand(len(lengths), -1).to(lengths.device)\n",
        "        length_masks = (ascending < lengths.unsqueeze(-1)).unsqueeze(-1)\n",
        "        return length_masks\n",
        "\n",
        "    def forward(self, inputs, lengths):\n",
        "        # the input shape should be (N, L, D∗H)\n",
        "        pad_mask = self._get_length_masks(lengths)\n",
        "        inputs = inputs * pad_mask\n",
        "        inputs = inputs.sum(dim=1)\n",
        "        out = self.classifier(inputs).squeeze()\n",
        "        return out"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdfQS_0FWbG3"
      },
      "source": [
        "def train(train_loader, backbone, header, optimizer, criterion, device, epoch):\n",
        "\n",
        "    total_loss = []\n",
        "    total_acc = []\n",
        "\n",
        "    for i, (idx_list, lengths, texts, labels) in enumerate(train_loader):\n",
        "        lengths, inputs, labels = lengths.to(device), texts.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        if not backbone is None:\n",
        "            inputs = backbone(inputs)\n",
        "        soft_predicted = header(inputs, lengths)\n",
        "        loss = criterion(soft_predicted, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            hard_predicted = (soft_predicted >= 0.5).int()\n",
        "            correct = sum(hard_predicted == labels).item()\n",
        "            batch_size = len(labels)\n",
        "\n",
        "            print('[Validation in epoch {:}] loss:{:.3f} acc:{:.3f}'.format(epoch+1, np.mean(total_loss), np.mean(total_acc)), end='\\r')\n",
        "    backbone.train()\n",
        "    header.train()\n",
        "    return np.mean(total_loss), np.mean(total_acc)\n",
        "\n",
        "\n",
        "def run_training(train_loader, valid_loader, backbone, header, epoch_num, lr, device, model_dir):\n",
        "    def check_point(backbone, header, loss, acc, model_dir):\n",
        "        # TODO\n",
        "        torch.save({'backbone': backbone, 'header': header}, model_dir)\n",
        "    def is_stop(loss, acc):\n",
        "        # TODO\n",
        "        return False\n",
        "        print('[ Epoch {}: {}/{} ] loss:{:.3f} acc:{:.3f} '.format(epoch+1, i+1, len(train_loader), loss.item(), correct * 100 / batch_size), end='\\r')\n",
        "\n",
        "def valid(valid_loader, backbone, header, criterion, device, epoch):\n",
        "    backbone.eval()\n",
        "    header.eval()\n",
        "    with torch.no_grad():\n",
        "        total_loss = []\n",
        "        total_acc = []\n",
        "\n",
        "        for i, (idx_list, lengths, texts, labels) in enumerate(valid_loader):\n",
        "            lengths, inputs, labels = lengths.to(device), texts.to(device), labels.to(device)\n",
        "\n",
        "            if not backbone is None:\n",
        "                inputs = backbone(inputs)\n",
        "            soft_predicted = header(inputs, lengths)\n",
        "            loss = criterion(soft_predicted, labels)\n",
        "            total_loss.append(loss.item())\n",
        "\n",
        "            hard_predicted = (soft_predicted >= 0.5).int()\n",
        "            correct = sum(hard_predicted == labels).item()\n",
        "            acc = correct * 100 / len(labels)\n",
        "            total_acc.append(acc)\n",
        "\n",
        "            print('[Validation in epoch {:}] loss:{:.3f} acc:{:.3f}'.format(epoch+1, np.mean(total_loss), np.mean(total_acc)), end='\\r')\n",
        "    backbone.train()\n",
        "    header.train()\n",
        "    return np.mean(total_loss), np.mean(total_acc)\n",
        "\n",
        "\n",
        "def run_training(train_loader, valid_loader, backbone, header, epoch_num, lr, device, model_dir):\n",
        "    def check_point(backbone, header, loss, acc, model_dir):\n",
        "        # TODO\n",
        "        torch.save({'backbone': backbone, 'header': header}, model_dir)\n",
        "    def is_stop(loss, acc):\n",
        "        # TODO\n",
        "        return False\n",
        "\n",
        "    if backbone is None:\n",
        "        trainable_paras = header.parameters()\n",
        "    else:\n",
        "        trainable_paras = list(backbone.parameters()) + list(header.parameters())\n",
        "\n",
        "    optimizer = torch.optim.Adam(trainable_paras, lr=lr)\n",
        "\n",
        "    backbone.train()\n",
        "    header.train()\n",
        "    backbone = backbone.to(device)\n",
        "    header = header.to(device)\n",
        "    criterion = torch.nn.BCELoss()\n",
        "    for epoch in range(epoch_num):\n",
        "        train(train_loader, backbone, header, optimizer, criterion, device, epoch)\n",
        "        loss, acc = valid(valid_loader, backbone, header, criterion, device, epoch)\n",
        "        print('[Validation in epoch {:}] loss:{:.3f} acc:{:.3f} '.format(epoch+1, loss, acc))\n",
        "        check_point(backbone, header, loss, acc, model_dir)\n",
        "        if is_stop(loss, acc):\n",
        "            break"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Amq9XWbEWbG9",
        "outputId": "e9229986-8bd1-497f-b96e-3d11e641aff4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "backbone = LSTM_Backbone(preprocessor.embedding_matrix, **lstm_config)\n",
        "header = Header(**header_config)\n",
        "\n",
        "run_training(train_loader, valid_loader, backbone, header, EPOCH_NUM, lr, device, MODEL_DIR)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Validation in epoch 1] loss:0.467 acc:77.723 \n",
            "[Validation in epoch 2] loss:0.452 acc:78.971 \n",
            "[Validation in epoch 3] loss:0.431 acc:79.824 \n",
            "[Validation in epoch 4] loss:0.424 acc:80.355 \n",
            "[Validation in epoch 5] loss:0.421 acc:80.251 \n",
            "[Validation in epoch 6] loss:0.418 acc:80.436 \n",
            "[Validation in epoch 7] loss:0.419 acc:80.658 \n",
            "[Validation in epoch 8] loss:0.433 acc:80.742 \n",
            "[Validation in epoch 9] loss:0.431 acc:80.438 \n",
            "[Validation in epoch 10] loss:0.430 acc:80.752 \n",
            "[Validation in epoch 11] loss:0.463 acc:79.803 \n",
            "[Validation in epoch 12] loss:0.526 acc:79.616 \n",
            "[Validation in epoch 13] loss:0.596 acc:78.883 \n",
            "[Validation in epoch 14] loss:0.678 acc:78.835 \n",
            "[Validation in epoch 15] loss:0.805 acc:78.970 \n",
            "[Validation in epoch 16] loss:0.900 acc:78.117 \n",
            "[Validation in epoch 17] loss:1.046 acc:78.187 \n",
            "[Validation in epoch 18] loss:1.036 acc:78.174 \n",
            "[Validation in epoch 19] loss:1.196 acc:78.058 \n",
            "[Validation in epoch 20] loss:1.312 acc:77.775 \n",
            "[Validation in epoch 21] loss:1.447 acc:78.254 \n",
            "[Validation in epoch 22] loss:1.323 acc:78.065 \n",
            "[Validation in epoch 23] loss:1.441 acc:78.337 \n",
            "[Validation in epoch 24] loss:1.803 acc:78.280 \n",
            "[Validation in epoch 25] loss:1.539 acc:77.873 \n",
            "[Validation in epoch 26] loss:1.787 acc:78.345 \n",
            "[Validation in epoch 27] loss:1.736 acc:78.481 \n",
            "[Validation in epoch 28] loss:1.569 acc:78.153 \n",
            "[Validation in epoch 29] loss:1.941 acc:78.597 \n",
            "[Validation in epoch 30] loss:1.893 acc:78.219 \n",
            "[Validation in epoch 31] loss:1.921 acc:77.799 \n",
            "[Validation in epoch 32] loss:1.931 acc:78.818 \n",
            "[Validation in epoch 33] loss:2.062 acc:78.216 \n",
            "[Validation in epoch 34] loss:1.976 acc:78.442 \n",
            "[Validation in epoch 35] loss:2.081 acc:78.722 \n",
            "[Validation in epoch 36] loss:2.019 acc:78.638 \n",
            "[Validation in epoch 37] loss:1.826 acc:77.996 \n",
            "[Validation in epoch 38] loss:1.936 acc:78.177 \n",
            "[Validation in epoch 39] loss:2.294 acc:78.228 \n",
            "[Validation in epoch 40] loss:2.151 acc:78.639 \n",
            "[Validation in epoch 41] loss:1.921 acc:78.483 \n",
            "[Validation in epoch 42] loss:1.764 acc:78.223 \n",
            "[Validation in epoch 43] loss:1.896 acc:78.044 \n",
            "[Validation in epoch 44] loss:1.949 acc:78.063 \n",
            "[Validation in epoch 45] loss:2.226 acc:78.774 \n",
            "[Validation in epoch 46] loss:2.138 acc:78.405 \n",
            "[Validation in epoch 47] loss:2.279 acc:78.397 \n",
            "[Validation in epoch 48] loss:2.013 acc:78.328 \n",
            "[Validation in epoch 49] loss:2.278 acc:78.983 \n",
            "[Validation in epoch 50] loss:1.997 acc:78.564 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfKCUNWRWbHi"
      },
      "source": [
        "def run_testing(test_loader, backbone, header, device, output_path):\n",
        "    with open(output_path, 'w') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['id', 'label'])\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, (idx_list, lengths, texts) in enumerate(test_loader):\n",
        "                lengths, inputs = lengths.to(device), texts.to(device)\n",
        "                if not backbone is None:\n",
        "                    inputs = backbone(inputs)\n",
        "                soft_predicted = header(inputs, lengths)\n",
        "                hard_predicted = (soft_predicted >= 0.5).int()\n",
        "                for i, p in zip(idx_list, hard_predicted):\n",
        "                    writer.writerow([str(i.item()), str(p.item())])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_09sRVgeWbIk",
        "outputId": "88a96493-df03-421a-ded0-ce7387d47551",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        }
      },
      "source": [
        "run_testing(test_loader, backbone, header, device, 'pred.csv')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-af2648cb8db6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_testing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackbone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pred.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-83e1a75b93f1>\u001b[0m in \u001b[0;36mrun_testing\u001b[0;34m(test_loader, backbone, header, device, output_path)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0midx_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m                 \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbackbone\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1344\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1345\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1372\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    692\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Caught IndexError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"<ipython-input-5-39f39bb9485d>\", line 76, in collate_fn\n    labels = torch.FloatTensor([d[2] for d in data])\n  File \"<ipython-input-5-39f39bb9485d>\", line 76, in <listcomp>\n    labels = torch.FloatTensor([d[2] for d in data])\nIndexError: tuple index out of range\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwTu77WNWbI3"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ql4DBilGWbI3"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}